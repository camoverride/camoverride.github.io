---
layout: post
title: Interviewing Participants
categories: [blog]
comments: true
---

User interviews are critically important in the software design process. I've recently been working on a lengthy scientfic experiment where participants learn bits of a new artificial language (a conlang) and then provide responses. These responses should lead them to acquire a knowledge of the spatial relationships of the conlang: they should have an intuitive knowledge that word X is associated with 'up' and word Z means something like 'to the side of.'

For the purposes of the experiment, we don't care whether the participants have learned the *correct* spatial categories in the conlang, only that they've learned some spatial relationships. So in the post-experiment interview I have to try and assess whether participants have learned spatial relationships or some other erroneous sort of relationship, such as accidentally associating the words with colors or shapes.

#### Bias

But I can't ask the participants directly whether they understand that they've been learning spatial relationships, as that would bias them. If you walk up to someone on March 20th and ask *"did you know that today is the Equinox?"*, they'll be inclined to say yes even if they didn't know anything about the equinox. Experimenters always introduce bias into in-person interviews, and reducing this bias is more of an art than a science.

During the interview process I have to carefully poke at the question of spatial relationships without ever using the word 'space'. Some of the things I'll ask will be "did you notice a connection between the words you were learning?" and "tell me about the meanings of some of the words you learned". Once a participant mentions learning a spatial category, they get marked as a successful learner. If they report erroenous categories or never mention space, their data is judged as unusable.

It's difficult conducting an entire study only to throw away data at the very end. As an experimenter, I'm biased to drop hints to a confused participant. Because every data point costs me time to gather, there's always a subconscious incentive to help the participant out. But I use a number of tricks to aid me during the questionairre process, such as a deadline (three minutes for the final question), and the avoidance of specific words ("space", "location").

These tricks make the interview process as scientific as possible. Post-experiment interviews are where I get feedback about the software, the user interface, and the actual learning experience. I think interviews give researchers a feel for the end-user experience and remind us that science is often messy.